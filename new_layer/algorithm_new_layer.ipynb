{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An algorithm to Learn Polytree Networks with Hidden nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:05.873437Z",
     "iopub.status.busy": "2024-03-02T21:32:05.873070Z",
     "iopub.status.idle": "2024-03-02T21:32:07.184276Z",
     "shell.execute_reply": "2024-03-02T21:32:07.184007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "from itertools import permutations\n",
    "import copy\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygraphviz\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import rpy2py\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:07.186301Z",
     "iopub.status.busy": "2024-03-02T21:32:07.186149Z",
     "iopub.status.idle": "2024-03-02T21:32:08.696809Z",
     "shell.execute_reply": "2024-03-02T21:32:08.696520Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Enable the conversion between Pandas DataFrames and R data.frames\n",
    "pandas2ri.activate()\n",
    "# Load and execute the R script\n",
    "result = ro.r.source(\"/Users/takeshine/Desktop/node_number/25/tree_data.R\")\n",
    "returned_list = result[0]\n",
    "# our synthetc data and edge list information for original polytree\n",
    "x_data = rpy2py(returned_list[0])  \n",
    "original_edge_data = rpy2py(returned_list[1])  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_data_path = \"/Users/takeshine/Desktop/new_layer/data_set/x_data.csv\"\n",
    "#original_edge_data_path = \"/Users/takeshine/Desktop/new_layer/data_set/original_edge_data.csv\" \n",
    "# sample_size = sample_size\n",
    "x_data = pd.read_csv(x_data_path)\n",
    "df_data = x_data.sample(n=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.698487Z",
     "iopub.status.busy": "2024-03-02T21:32:08.698403Z",
     "iopub.status.idle": "2024-03-02T21:32:08.713531Z",
     "shell.execute_reply": "2024-03-02T21:32:08.713257Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_data = pd.DataFrame(x_data)\n",
    "df_data.columns = [f'V{i+1}' for i in range(len(df_data.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.714874Z",
     "iopub.status.busy": "2024-03-02T21:32:08.714777Z",
     "iopub.status.idle": "2024-03-02T21:32:08.718652Z",
     "shell.execute_reply": "2024-03-02T21:32:08.718353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the number of columns\n",
    "n = df_data.shape[1]\n",
    "# Create a mapping of old column names to new column names\n",
    "new_columns = {f\"V{i+1}\": f\"y{i+1}\" for i in range(n)}\n",
    "\n",
    "# Rename the columns\n",
    "df_data = df_data.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.719995Z",
     "iopub.status.busy": "2024-03-02T21:32:08.719926Z",
     "iopub.status.idle": "2024-03-02T21:32:08.723611Z",
     "shell.execute_reply": "2024-03-02T21:32:08.723381Z"
    }
   },
   "outputs": [],
   "source": [
    "number_sample = df_data.shape[0]\n",
    "alpha = 0.1  # Significance level\n",
    "# Degrees of freedom\n",
    "df = number_sample - 2\n",
    "# Get critical t-value (two-tailed test)\n",
    "critical_t_value = t.ppf(1 - alpha/2, df)\n",
    "\n",
    "# Convert t-value to r (correlation coefficient)\n",
    "threshold_corr = critical_t_value / (critical_t_value**2 + df)**0.5\n",
    "threshold_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.724710Z",
     "iopub.status.busy": "2024-03-02T21:32:08.724633Z",
     "iopub.status.idle": "2024-03-02T21:32:08.872707Z",
     "shell.execute_reply": "2024-03-02T21:32:08.872443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the original correlation matrix\n",
    "original_correlation_matrix = df_data.corr()\n",
    "\n",
    "# Use the `where` function to replace values, we use 0.05 as our cut\n",
    "new_correlation_matrix  = df_data.corr().applymap(lambda x: 0 if abs(x) < threshold_corr else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Pairwise-Finite Distance Algorithm (PFDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.874245Z",
     "iopub.status.busy": "2024-03-02T21:32:08.874148Z",
     "iopub.status.idle": "2024-03-02T21:32:08.877343Z",
     "shell.execute_reply": "2024-03-02T21:32:08.877063Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise_finite_distance_algorithm(new_correlation_matrix):\n",
    "\n",
    "    new_correlation_matrix = new_correlation_matrix.to_numpy()\n",
    "    n = new_correlation_matrix.shape[0] # Get the number of nodes\n",
    "\n",
    "    nodes = [f\"y{i + 1}\" for i in range(n)]\n",
    "    sets = {} \n",
    "\n",
    "    # Step 1 - 3: for each node, if it's independent from all other nodes, add it to a set\n",
    "    for i, node in enumerate(nodes):\n",
    "        if all(new_correlation_matrix[i, np.arange(n) != i]== 0):\n",
    "            sets[f'S_{i+1},0'] = {node}\n",
    "\n",
    "    # Step 4 - 9 For each pair of nodes, if they are dependent, add them to a set\n",
    "    for i, node in enumerate(nodes):\n",
    "        for j, other in enumerate(nodes):\n",
    "            if i < j and new_correlation_matrix[i,j] == 1:\n",
    "                sets[f'S_{i+1},{j+1}'] = {node, other}\n",
    "                while True:  # Keep adding nodes to the set as long as there exist nodes that are dependent on all nodes in the set\n",
    "                    old_size = len(sets[f'S_{i+1},{j+1}'])\n",
    "\n",
    "                    # For each other node, if it's dependent on all nodes in the set, add it to the set\n",
    "                    for k, another in enumerate(nodes):\n",
    "                        if k != i and k != j and all(new_correlation_matrix[k,nodes.index(n)] == 1 for n in sets[f'S_{i+1},{j+1}']):\n",
    "                            sets[f'S_{i+1},{j+1}'].add(another)\n",
    "                    if len(sets[f'S_{i+1},{j+1}']) == old_size:\n",
    "                        break\n",
    "                    \n",
    "    # Step 10: Create a new dictionary to store the unique sets\n",
    "    unique_sets = {}\n",
    "    for key, value in sets.items():\n",
    "        # Convert the set to a frozenset\n",
    "        frozenset_value = frozenset(value)\n",
    "        # Only add the set to the dictionary if it's not already in the dictionary\n",
    "        if frozenset_value not in unique_sets.values():\n",
    "            unique_sets[key] = frozenset_value\n",
    "\n",
    "    # Convert the frozensets back to sets\n",
    "    unique_sets = {key: set(value) for key, value in unique_sets.items()}\n",
    "\n",
    "    return unique_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.878464Z",
     "iopub.status.busy": "2024-03-02T21:32:08.878388Z",
     "iopub.status.idle": "2024-03-02T21:32:08.882595Z",
     "shell.execute_reply": "2024-03-02T21:32:08.882382Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_sets =  pairwise_finite_distance_algorithm(new_correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.883732Z",
     "iopub.status.busy": "2024-03-02T21:32:08.883668Z",
     "iopub.status.idle": "2024-03-02T21:32:08.885449Z",
     "shell.execute_reply": "2024-03-02T21:32:08.885232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename the output sets of task1 \n",
    "task1_sets =  {}\n",
    "# Interate through the unique sets, renaming them and optionally reordering the elements, preparing for task2 \n",
    "for i, (key, value) in enumerate(unique_sets.items()):\n",
    "    new_name = f'S_{i + 1}'\n",
    "    sorted_value = sorted(value, key=lambda x: int(x[1:]))\n",
    "    task1_sets[new_name] = sorted_value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Reconstruction Algorithm for Latent Rooted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare inputs for Task 2\n",
    "\n",
    "- Use partial correlations to represent conditonal independence statements\n",
    "- Use linear regression to calcluate partial correlation:https://en.wikipedia.org/wiki/Partial_correlation\n",
    "- we use t test to determine the cut out value for partial correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.886575Z",
     "iopub.status.busy": "2024-03-02T21:32:08.886510Z",
     "iopub.status.idle": "2024-03-02T21:32:08.888272Z",
     "shell.execute_reply": "2024-03-02T21:32:08.888029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a global variable-to-index mapping\n",
    "all_variables = [f'y{i}' for i in range(1, n+1)] # recall n is number of columns for df_data\n",
    "var_to_idx = {var: idx for idx, var in enumerate(all_variables)}\n",
    "global_partial_correlations = np.full((n,n,n),np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Explanation of Partial Correlation Calculation\n",
    "\n",
    "This code calculates the partial correlations between pairs of variables while controlling for a third variable in a dataset.\n",
    "\n",
    "1. **Partial Correlation**: \n",
    "   - Measures the correlation between two variables, controlling for one or more additional variables.\n",
    "\n",
    "2. **Linear Regression for Residuals**: \n",
    "   - Linear regression models are fitted for each variable against the control variable.\n",
    "   - Residuals (observed - predicted values) are then used for further calculation.\n",
    "\n",
    "3. **Correlation of Residuals**: \n",
    "   - The partial correlation is calculated as the correlation between the residuals of two variables.\n",
    "\n",
    "4. **Special Cases Handling**: \n",
    "   - If only one variable is present, partial correlation is 0.\n",
    "   - With two variables and no control, it's the Pearson correlation.\n",
    "\n",
    "5. **Storing Results**: \n",
    "   - Partial correlations are stored in a 3D array, indexed by the involved variables.\n",
    "\n",
    "6. **Handling NaNs and Diagonal Values**: \n",
    "   - NaNs in the array are set to 0.\n",
    "   - Diagonal values in the first two dimensions are set to 1 (correlation of a variable with itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.889417Z",
     "iopub.status.busy": "2024-03-02T21:32:08.889345Z",
     "iopub.status.idle": "2024-03-02T21:32:08.892270Z",
     "shell.execute_reply": "2024-03-02T21:32:08.892069Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_partial_correlations_for_triplets(data,variables):\n",
    "\n",
    "    '''\n",
    "    data as dataframe \n",
    "    varibles should be columns of those samples we generated for each varible \n",
    "    '''\n",
    "\n",
    "    # Iterate over all combinations of var1 and var2\n",
    "    for var1, var2 in permutations(variables, 2):\n",
    "        if len(variables) == 1:\n",
    "            partial_correlation = 0 \n",
    "        elif len(variables) == 2: \n",
    "            partial_correlation = data[var1].corr(data[var2])\n",
    "        else:\n",
    "            # For each combination of var1 and var2, iterate over the remaining variables as control_var\n",
    "            for control_var in [var for var in variables if var != var1 and var != var2]:\n",
    "                \n",
    "                # Fit the linear regression model for var1 controlled by control_var\n",
    "                model_var1 = sm.OLS(data[var1], sm.add_constant(data[control_var])).fit()\n",
    "                residuals_var1 = model_var1.resid\n",
    "\n",
    "                # Fit the linear regression model for var2 controlled by control_var\n",
    "                model_var2 = sm.OLS(data[var2], sm.add_constant(data[control_var])).fit()\n",
    "                residuals_var2 = model_var2.resid\n",
    "\n",
    "                if np.std(residuals_var1) != 0 and np.std(residuals_var2) != 0:\n",
    "                # Compute the correlation between the residuals\n",
    "                    partial_correlation = residuals_var1.corr(residuals_var2)\n",
    "                else:\n",
    "                    partial_correlation = 0  # Set a default value\n",
    "                # Store the result in the 3D array\n",
    "\n",
    "                i, j, k = var_to_idx[var1], var_to_idx[var2], var_to_idx[control_var]\n",
    "                global_partial_correlations[i, j, k] = partial_correlation\n",
    "\n",
    "    # let us set nans to 0 first \n",
    "    global_partial_correlations[np.isnan(global_partial_correlations)] = 0\n",
    "\n",
    "    # diagoal on first 2D should all be one\n",
    "    for i in range(global_partial_correlations.shape[0]):\n",
    "        for k in range(global_partial_correlations.shape[2]):\n",
    "            global_partial_correlations[i,i,k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:08.893489Z",
     "iopub.status.busy": "2024-03-02T21:32:08.893419Z",
     "iopub.status.idle": "2024-03-02T21:32:16.269900Z",
     "shell.execute_reply": "2024-03-02T21:32:16.269244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the partial correlations for each set\n",
    "for set_name, variables in task1_sets.items():\n",
    "    calculate_partial_correlations_for_triplets(df_data, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the cut for partial correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.274060Z",
     "iopub.status.busy": "2024-03-02T21:32:16.273636Z",
     "iopub.status.idle": "2024-03-02T21:32:16.321784Z",
     "shell.execute_reply": "2024-03-02T21:32:16.321135Z"
    }
   },
   "outputs": [],
   "source": [
    "check_list = []\n",
    "for i in range(global_partial_correlations.shape[0]):\n",
    "    for j in range(global_partial_correlations.shape[1]):\n",
    "        for k in range(global_partial_correlations.shape[2]):\n",
    "            if (global_partial_correlations[i,j,k] != 1 and \n",
    "            global_partial_correlations[i,j,k] != 0 and \n",
    "            abs(global_partial_correlations[i,j,k])) < 0.3:\n",
    "                check_list.append(abs(global_partial_correlations[i,j,k]))\n",
    "print(len(check_list))\n",
    "check_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.326123Z",
     "iopub.status.busy": "2024-03-02T21:32:16.325904Z",
     "iopub.status.idle": "2024-03-02T21:32:16.336690Z",
     "shell.execute_reply": "2024-03-02T21:32:16.333354Z"
    }
   },
   "outputs": [],
   "source": [
    "def partial_corr_significance(r, n, k):\n",
    "    # Compute the t-statistic\n",
    "    t_stat = r * np.sqrt((n - k - 2) / (1 - r**2))\n",
    "    \n",
    "    # Compute the one-tailed p-value (for positive correlation)\n",
    "    degree_of_freedom = n - k - 2\n",
    "    p_value = 1 - stats.t.cdf(abs(t_stat), degree_of_freedom)\n",
    "    \n",
    "    return t_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.343677Z",
     "iopub.status.busy": "2024-03-02T21:32:16.341098Z",
     "iopub.status.idle": "2024-03-02T21:32:16.816349Z",
     "shell.execute_reply": "2024-03-02T21:32:16.816028Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = df_data.shape[0] # Sample size\n",
    "k = 1    # Number of control variables\n",
    "p_value_paramater =0.05\n",
    "for partial_corr in check_list:\n",
    "    t_stat, p_value = partial_corr_significance(partial_corr, sample_size, k)\n",
    "    if p_value < p_value_paramater: \n",
    "        #print(f\"t-statistic: {t_stat:.4f}\")\n",
    "        \n",
    "        # print(f\"p-value: {p_value:.4f}\")\n",
    "        # print(\"Choose cut near \" + str(partial_corr) )\n",
    "        threshold = partial_corr\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.817753Z",
     "iopub.status.busy": "2024-03-02T21:32:16.817677Z",
     "iopub.status.idle": "2024-03-02T21:32:16.819393Z",
     "shell.execute_reply": "2024-03-02T21:32:16.819142Z"
    }
   },
   "outputs": [],
   "source": [
    "# We need to set entries whose absolute value is smaller our threshold to zero\n",
    "global_partial_correlations[np.abs(global_partial_correlations) < threshold] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 2: determine the collapased quasi-skeleton of each rooted subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.820692Z",
     "iopub.status.busy": "2024-03-02T21:32:16.820626Z",
     "iopub.status.idle": "2024-03-02T21:32:16.823055Z",
     "shell.execute_reply": "2024-03-02T21:32:16.822818Z"
    }
   },
   "outputs": [],
   "source": [
    "def step3_find_terminal_node(V_temp, global_partial_correlations, variable_to_index):\n",
    "    \"\"\"\n",
    "    Determine a visible terminal node in V_temp.\n",
    "\n",
    "    Parameters:\n",
    "    - V_temp: Current set of nodes.\n",
    "    - global_partial_correlations: 3D numpy array of partial correlations.\n",
    "    - variable_to_index: Dictionary mapping node names to indices.\n",
    "\n",
    "    Returns:\n",
    "    - terminal_node: The node with the least number of violating pairs.\n",
    "    \"\"\"\n",
    "    violations_count = {}  # This dictionary will store count of violations for each node_k\n",
    "    \n",
    "    for node_k in V_temp:\n",
    "        idx_k = variable_to_index[node_k]\n",
    "        excluded_nodes = set(V_temp) - {node_k}\n",
    "        count = 0\n",
    "        for (node_i, node_j) in permutations(excluded_nodes, 2):\n",
    "            if node_i != node_k and node_j != node_k:\n",
    "                idx_i, idx_j = variable_to_index[node_i], variable_to_index[node_j]\n",
    "                if global_partial_correlations[idx_i, idx_k, idx_j] != 0:\n",
    "                    count += 1  # Count this as a violation for node_k\n",
    "        violations_count[node_k] = count\n",
    "    \n",
    "    # Return the node with the least violations\n",
    "    terminal_node = min(violations_count, key=violations_count.get)\n",
    "    return terminal_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.824181Z",
     "iopub.status.busy": "2024-03-02T21:32:16.824104Z",
     "iopub.status.idle": "2024-03-02T21:32:16.826347Z",
     "shell.execute_reply": "2024-03-02T21:32:16.826097Z"
    }
   },
   "outputs": [],
   "source": [
    "def step4_find_linked_node(terminal_node, V_temp, global_partial_correlations, var_to_idx):\n",
    "    linked_node = None\n",
    "\n",
    "    if terminal_node:\n",
    "        idx_k = var_to_idx[terminal_node]\n",
    "        found_node_l = False\n",
    "        excluded_nodes_1 = set(V_temp) - {terminal_node}\n",
    "        for node_l in excluded_nodes_1:\n",
    "            idx_l = var_to_idx[node_l]\n",
    "            is_linked = True\n",
    "            for node_j in excluded_nodes_1:\n",
    "                idx_j = var_to_idx[node_j]\n",
    "                if idx_j != idx_l and global_partial_correlations[idx_k, idx_l, idx_j] == 0:\n",
    "                    is_linked = False\n",
    "                    break\n",
    "            if is_linked:\n",
    "                linked_node = node_l\n",
    "                break\n",
    "\n",
    "    # Return the updated V_temp and the linked_node\n",
    "    return linked_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.827541Z",
     "iopub.status.busy": "2024-03-02T21:32:16.827476Z",
     "iopub.status.idle": "2024-03-02T21:32:16.829520Z",
     "shell.execute_reply": "2024-03-02T21:32:16.829300Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_set_K(V_temp, terminal_node, global_partial_correlations, var_to_idx):\n",
    "    K = []\n",
    "    while True:\n",
    "        K_updated = False\n",
    "        for node_i, node_j in permutations(V_temp, 2):\n",
    "            if node_i != node_j and node_i not in K:\n",
    "                idx_i, idx_j = var_to_idx[node_i], var_to_idx[node_j]\n",
    "                idx_k = var_to_idx[terminal_node]\n",
    "                if global_partial_correlations[idx_j, idx_i, idx_k] != 0 and node_j not in K:\n",
    "                    K.append(node_j)\n",
    "                    K_updated = True\n",
    "                    break\n",
    "        if not K_updated:\n",
    "            break\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.830631Z",
     "iopub.status.busy": "2024-03-02T21:32:16.830566Z",
     "iopub.status.idle": "2024-03-02T21:32:16.833611Z",
     "shell.execute_reply": "2024-03-02T21:32:16.833383Z"
    }
   },
   "outputs": [],
   "source": [
    "def latent_tree_structure_prepare(set_of_nodes, global_partial_correlations, var_to_idx):\n",
    "    edges = []\n",
    "    latent_nodes = []\n",
    "    V_temp = list(set_of_nodes)  # Convert to list for indexing.\n",
    "    candidates = {}\n",
    "    K = None # Initialize K as none \n",
    "\n",
    "    while True:\n",
    "        # Step 2a: Check if V_temp has only two nodes\n",
    "        if len(V_temp) == 2:\n",
    "            edges.append((V_temp[0], V_temp[1]))\n",
    "            break\n",
    "        \n",
    "        # Step 2b: If more than 2 nodes, proceed to find the terminal node\n",
    "        terminal_node = step3_find_terminal_node(V_temp, global_partial_correlations, var_to_idx)\n",
    "        # print(terminal_node)\n",
    "        \n",
    "        # Step 4: Check for Linked Node\n",
    "        linked_node = step4_find_linked_node(terminal_node, V_temp, global_partial_correlations, var_to_idx)\n",
    "        \n",
    "        if linked_node:  # If a linked node exists\n",
    "            edges.append((linked_node, terminal_node))\n",
    "            V_temp.remove(terminal_node)\n",
    "        else:  # If no linked node exists\n",
    "            new_latent = \"L\" + str(len(latent_nodes) + 1)\n",
    "            latent_nodes.append(new_latent)\n",
    "            edges.append((new_latent, terminal_node))\n",
    "            V_temp.remove(terminal_node)\n",
    "            \n",
    "            K = compute_set_K(V_temp, terminal_node, global_partial_correlations, var_to_idx)\n",
    "            if not K:\n",
    "                return K,candidates, edges, latent_nodes\n",
    "\n",
    "            else: \n",
    "                for node_j in K:\n",
    "                    edges.append((node_j, new_latent))\n",
    "                    idx_j = var_to_idx[node_j]\n",
    "                    idx_k = var_to_idx[terminal_node]\n",
    "                    candidates[node_j] = [node_j]\n",
    "                    for node_i in V_temp:\n",
    "                        if node_i != node_j and node_i != terminal_node and node_j != terminal_node:\n",
    "                            idx_i = var_to_idx[node_i]\n",
    "                            if global_partial_correlations[idx_k, idx_j, idx_i] == 0:\n",
    "                                candidates[node_j].append(node_i)\n",
    "            break  # Exit the loop after processing when no linked node exists\n",
    "    return K, candidates, edges, latent_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.834736Z",
     "iopub.status.busy": "2024-03-02T21:32:16.834667Z",
     "iopub.status.idle": "2024-03-02T21:32:16.838329Z",
     "shell.execute_reply": "2024-03-02T21:32:16.838107Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_final_latent_structures(sets, global_partial_correlations, var_to_idx):\n",
    "    all_edges = {}\n",
    "    all_latent_nodes = {}\n",
    "\n",
    "    for key, value in sets.items():\n",
    "        # Initial edges and latent nodes\n",
    "        _, candidates, initial_edges, initial_latent_nodes = latent_tree_structure_prepare(\n",
    "            value, global_partial_correlations\n",
    "            , var_to_idx\n",
    "            )\n",
    "        \n",
    "        # Rename the initial latent nodes\n",
    "        renamed_initial_latent_nodes = [\n",
    "            \"L_\" + key.split('_')[-1] + \"_\" + str(idx) \n",
    "            for idx, _ in enumerate(initial_latent_nodes, 1)]\n",
    "        rename_dict_initial = dict(zip(initial_latent_nodes, renamed_initial_latent_nodes))\n",
    "        renamed_initial_edges = [\n",
    "            (rename_dict_initial[edge[0]] if edge[0] in rename_dict_initial else edge[0], \n",
    "                                  rename_dict_initial[edge[1]] if edge[1] in rename_dict_initial else edge[1]) \n",
    "                                  for edge in initial_edges]\n",
    "        \n",
    "        # Temporary storage for edges and latent nodes for each candidate set\n",
    "        temp_edges = []\n",
    "        temp_latent_nodes = []\n",
    "\n",
    "        # If the candidate set is not empty, proceed to iterate over its values\n",
    "        if candidates:\n",
    "            for candidate_key, candidate_value in candidates.items():\n",
    "                # Edges and latent nodes for the current candidate set\n",
    "                _, _, candidate_edges, candidate_latent_nodes = latent_tree_structure_prepare(candidate_value, global_partial_correlations, var_to_idx)\n",
    "                \n",
    "                # Renaming the latent nodes based on the main set's index and the candidate set's index\n",
    "                renamed_candidate_latent_nodes = [\n",
    "                    \"L_\" + key.split('_')[-1] + \"_\" + candidate_key + \"_\" + str(idx) \n",
    "                    for idx, _ in enumerate(candidate_latent_nodes, 1)\n",
    "                    ]\n",
    "                rename_dict_candidate = dict(zip(candidate_latent_nodes, renamed_candidate_latent_nodes))\n",
    "                renamed_candidate_edges = [\n",
    "                    (rename_dict_candidate[edge[0]] if edge[0] in rename_dict_candidate else edge[0], \n",
    "                                           rename_dict_candidate[edge[1]] if edge[1] in rename_dict_candidate else edge[1]) \n",
    "                                           for edge in candidate_edges\n",
    "                                           ]\n",
    "                \n",
    "                temp_edges.extend(renamed_candidate_edges)\n",
    "                temp_latent_nodes.extend(renamed_candidate_latent_nodes)\n",
    "\n",
    "        # Combine initial edges and latent nodes with those obtained from the candidate sets\n",
    "        combined_edges = renamed_initial_edges + temp_edges\n",
    "        combined_latent_nodes = renamed_initial_latent_nodes + temp_latent_nodes\n",
    "        \n",
    "        # Store the combined edges and latent nodes\n",
    "        all_edges[\"E_\" + key.split('_')[-1]] = combined_edges\n",
    "        all_latent_nodes[\"latent_nodes_\" + key.split('_')[-1]] = combined_latent_nodes\n",
    "\n",
    "        \"\"\"\n",
    "        Remove duplicate edges from the edges dictionary.\n",
    "        \"\"\"\n",
    "        # Using set to automatically remove duplicates and then converting back to list\n",
    "        cleaned_edges = {key: list(set(value)) for key, value in all_edges.items()}\n",
    "\n",
    "    return cleaned_edges, all_latent_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.839515Z",
     "iopub.status.busy": "2024-03-02T21:32:16.839448Z",
     "iopub.status.idle": "2024-03-02T21:32:16.841276Z",
     "shell.execute_reply": "2024-03-02T21:32:16.841066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the function\n",
    "cleaned_final_edges, all_latent_nodes = generate_final_latent_structures(task1_sets, global_partial_correlations, var_to_idx)\n",
    "# cleaned_final_edges, all_latent_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine nodes, edges and latent nodes for each subtree\n",
    "- collapse the latent nodes to hidden cluster and clean the edges\n",
    "- visulize the result for task 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.842417Z",
     "iopub.status.busy": "2024-03-02T21:32:16.842355Z",
     "iopub.status.idle": "2024-03-02T21:32:16.844200Z",
     "shell.execute_reply": "2024-03-02T21:32:16.843985Z"
    }
   },
   "outputs": [],
   "source": [
    "subtrees = {}  \n",
    "for i in range(1, len(task1_sets) + 1):\n",
    "\n",
    "    set_key = 'S_' + str(i)\n",
    "    edge_key = 'E_' + str(i)\n",
    "    latent_node_key = 'latent_nodes_' + str(i)\n",
    "        \n",
    "    subtree = {\n",
    "        \"nodes\": task1_sets[set_key],\n",
    "        \"edges\": cleaned_final_edges[edge_key],\n",
    "        \"latent_nodes\": all_latent_nodes[latent_node_key]\n",
    "    }\n",
    "    subtrees['T_' + str(i)] = subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.845280Z",
     "iopub.status.busy": "2024-03-02T21:32:16.845220Z",
     "iopub.status.idle": "2024-03-02T21:32:16.847441Z",
     "shell.execute_reply": "2024-03-02T21:32:16.847201Z"
    }
   },
   "outputs": [],
   "source": [
    "collapsed_subtrees = {}\n",
    "latent_cluster_counter = 1  # Counter for the latent cluster name\n",
    "\n",
    "for key, subtree in subtrees.items():\n",
    "    # Create a new name for the latent cluster based on the counter if there are latent nodes in the subtree\n",
    "    latent_cluster_name = f\"C_{latent_cluster_counter}\" if subtree['latent_nodes'] else \"\"\n",
    "    \n",
    "    # Create a new list of nodes, removing the old latent nodes and adding the new cluster node\n",
    "    new_nodes = [node for node in subtree['nodes'] if node not in subtree['latent_nodes']]\n",
    "    if subtree['latent_nodes']:  # Only add the cluster node if there were latent nodes to begin with\n",
    "        new_nodes.append(latent_cluster_name)\n",
    "        latent_cluster_counter += 1  # Increment the counter for the next latent cluster\n",
    "    \n",
    "    # Update the edges, replacing any old latent nodes with the new cluster node\n",
    "    new_edges = []\n",
    "    for edge in subtree['edges']:\n",
    "        new_edge = tuple([latent_cluster_name if node in subtree['latent_nodes'] else node for node in edge])\n",
    "        new_edges.append(new_edge)\n",
    "    \n",
    "    collapsed_subtrees[key] = {\n",
    "        'nodes': new_nodes,\n",
    "        'edges': new_edges,\n",
    "        'latent_clusters': [latent_cluster_name] if subtree['latent_nodes'] else []\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all subtrees in one big paper with proper distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.848713Z",
     "iopub.status.busy": "2024-03-02T21:32:16.848649Z",
     "iopub.status.idle": "2024-03-02T21:32:16.851321Z",
     "shell.execute_reply": "2024-03-02T21:32:16.851084Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_all_subtrees_with_axis_limits(ax, subtree, title, scale_factor=1.2):\n",
    "    \"\"\"\n",
    "    Visualize the given subtree on a specific axis using networkx and matplotlib.\n",
    "    Scale the layout to adjust edge lengths and set axis limits for clear separation.\n",
    "    \n",
    "    Parameters:\n",
    "    - ax: Axis on which to plot the subtree.\n",
    "    - subtree: Dictionary containing nodes, edges, and latent nodes information.\n",
    "    - title: Title for the visualization.\n",
    "    - scale_factor: Factor to scale the layout and adjust edge lengths.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for node in subtree['nodes']:\n",
    "        if node in subtree['latent_clusters']:\n",
    "            G.add_node(node, color='#FFC0CB')  # Pink color for latent nodes\n",
    "        else:\n",
    "            G.add_node(node, color='#AEDFF7')  # Light blue color for regular nodes\n",
    "\n",
    "    G.add_edges_from(subtree['edges'])\n",
    "    \n",
    "    # Use kamada_kawai_layout for better spacing between nodes\n",
    "    layout = nx.kamada_kawai_layout(G)\n",
    "    \n",
    "    # Adjust the node positions by scaling the layout to adjust edge lengths\n",
    "    for node, (x, y) in layout.items():\n",
    "        layout[node] = (scale_factor * x, scale_factor * y)\n",
    "    \n",
    "    colors = [G.nodes[node]['color'] for node in G.nodes()]\n",
    "    nx.draw(G, pos=layout, with_labels=True, node_color=colors, node_size=500, \n",
    "            font_size=10, font_weight='bold', edge_color='#A9A9A9', ax=ax, \n",
    "            width=1.3)  # Thicker edges for clarity\n",
    "    ax.set_title(title, fontsize=12)  # Adjusted font size for title\n",
    "\n",
    "    # Set the axis' background color\n",
    "    ax.set_facecolor('#F5F5F5')  # Light gray background\n",
    "    \n",
    "    # Set axis limits to create a boundary around each graph\n",
    "    ax.set_xlim([-2, 2]) # biggger the number, smaller the graph\n",
    "    ax.set_ylim([-2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:16.852564Z",
     "iopub.status.busy": "2024-03-02T21:32:16.852498Z",
     "iopub.status.idle": "2024-03-02T21:32:17.399208Z",
     "shell.execute_reply": "2024-03-02T21:32:17.398911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a large figure with 5x4 grid\n",
    "fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(18, 18))\n",
    "fig.subplots_adjust(hspace=0.8, wspace=0.8)  # Increased spacing between subplots\n",
    "\n",
    "# Visualize each subtree with adjusted edge lengths and axis limits\n",
    "for ax, (key, subtree) in zip(axes.ravel(), collapsed_subtrees.items()):\n",
    "    visualize_all_subtrees_with_axis_limits(ax, subtree, key)\n",
    "\n",
    "# Hide any remaining unused subplots (since we're using a 5x4 grid for 16 subtrees)\n",
    "for i in range(len(collapsed_subtrees), 5*4):\n",
    "    axes.ravel()[i].axis('off')\n",
    "\n",
    "# display\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"axis_limits_subtrees.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Categorizing subtrees into with/without hidden nodes\n",
    "- merge substree without hidden nodes first cause no merging crieria needed\n",
    "- merge subtress with hidden nodes, follow the merging crieria \n",
    "- make sure in the end there is no cycles in our final quasi-skeleton of the latent polytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.400560Z",
     "iopub.status.busy": "2024-03-02T21:32:17.400490Z",
     "iopub.status.idle": "2024-03-02T21:32:17.403352Z",
     "shell.execute_reply": "2024-03-02T21:32:17.403130Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_subtree(subtree):\n",
    "    \"\"\"\n",
    "    Visualize the given subtree using the networkx and matplotlib libraries.\n",
    "    Nodes are colored based on their type (latent or visible), and edges are shown connecting the nodes.\n",
    "    \n",
    "    Parameters:\n",
    "    - subtree: A dictionary containing nodes, edges, and latent_clusters of the subtree.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Ensure all nodes present in edges are in the 'nodes' list\n",
    "    all_nodes = set(subtree['nodes'])\n",
    "    for edge in subtree['edges']:\n",
    "        all_nodes.update(edge)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    for node in all_nodes:\n",
    "        if node in subtree['latent_clusters']:\n",
    "            G.add_node(node, color='#FFC0CB')  # Pink color for latent nodes\n",
    "        else:\n",
    "            G.add_node(node, color='#AEDFF7')  # Light blue color for regular nodes\n",
    "\n",
    "    # Add edges to the graph\n",
    "    G.add_edges_from(subtree['edges'])\n",
    "    \n",
    "    # Use kamada_kawai_layout for better spacing between nodes\n",
    "    layout = nx.kamada_kawai_layout(G)\n",
    "    \n",
    "    colors = [G.nodes[node]['color'] for node in G.nodes()]\n",
    "    nx.draw(G, pos=layout, with_labels=True, node_color=colors, node_size=500, \n",
    "            font_size=10, font_weight='bold', edge_color='#A9A9A9', ax=ax, \n",
    "            width=1.3)  # Thicker edges for clarity\n",
    "    ax.set_title(\"Subtree\", fontsize=12)  # Adjusted font size for title\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.404498Z",
     "iopub.status.busy": "2024-03-02T21:32:17.404433Z",
     "iopub.status.idle": "2024-03-02T21:32:17.406310Z",
     "shell.execute_reply": "2024-03-02T21:32:17.406091Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorizing_subtrees(subtrees):\n",
    "    subtree_with_hidden_clusters = {}\n",
    "    subtree_without_hidden_clusters = {}\n",
    "\n",
    "    for key, subtree in subtrees.items():\n",
    "        if subtree['latent_clusters']:\n",
    "            subtree_with_hidden_clusters[key] = subtree\n",
    "            # Remove latent clusters from the nodes list\n",
    "            for cluster in subtree['latent_clusters']:\n",
    "                subtree['nodes'].remove(cluster)\n",
    "        else:\n",
    "            subtree_without_hidden_clusters[key] = subtree\n",
    "            \n",
    "    return subtree_with_hidden_clusters, subtree_without_hidden_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.407459Z",
     "iopub.status.busy": "2024-03-02T21:32:17.407393Z",
     "iopub.status.idle": "2024-03-02T21:32:17.408966Z",
     "shell.execute_reply": "2024-03-02T21:32:17.408715Z"
    }
   },
   "outputs": [],
   "source": [
    "subtree_with_hidden_clusters, subtree_without_hidden_clusters = categorizing_subtrees(collapsed_subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.410059Z",
     "iopub.status.busy": "2024-03-02T21:32:17.409989Z",
     "iopub.status.idle": "2024-03-02T21:32:17.411870Z",
     "shell.execute_reply": "2024-03-02T21:32:17.411666Z"
    }
   },
   "outputs": [],
   "source": [
    "def merged_subtrees_without_hidden(subtrees_without_hidden):\n",
    "    combined_nodes = []\n",
    "    combined_edges = []\n",
    "\n",
    "    # Iterate through each subtree and collect nodes and edges\n",
    "    for _, subtree in subtrees_without_hidden.items():\n",
    "        combined_nodes.extend(subtree['nodes'])\n",
    "        combined_edges.extend(subtree['edges'])\n",
    "\n",
    "    # Convert to set and back to list to ensure uniqueness \n",
    "    combined_nodes = list(set(combined_nodes))\n",
    "    combined_edges = list(set(combined_edges))\n",
    "    \n",
    "    # Return the combined subtree\n",
    "    return{\n",
    "            \"nodes\": combined_nodes,\n",
    "            \"edges\": combined_edges,\n",
    "            \"latent_clusters\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.413099Z",
     "iopub.status.busy": "2024-03-02T21:32:17.413019Z",
     "iopub.status.idle": "2024-03-02T21:32:17.416082Z",
     "shell.execute_reply": "2024-03-02T21:32:17.415853Z"
    }
   },
   "outputs": [],
   "source": [
    "def merging_criteria(subtree1, subtree2):\n",
    "    # Extracting hidden clusters\n",
    "    cluster1 = subtree1['latent_clusters'][0] if subtree1['latent_clusters'] else None \n",
    "    cluster2 = subtree2['latent_clusters'][0] if subtree2['latent_clusters'] else None \n",
    "\n",
    "    # If either of the subtrees doesn't have a hidden cluster, merging is not possible  \n",
    "    if not cluster1 or not cluster2:\n",
    "        return subtree1, subtree2, False\n",
    "    \n",
    "    # Find neighbors of hidden clusters in borh subtrees\n",
    "    neighbors_cluster1 = {edge[1] for edge in subtree1['edges'] if edge[0] == cluster1} | \\\n",
    "                         {edge[0] for edge in subtree1['edges'] if edge[1] == cluster1}\n",
    "    neighbors_cluster2 = {edge[1] for edge in subtree2['edges'] if edge[0] == cluster2} | \\\n",
    "                         {edge[0] for edge in subtree2['edges'] if edge[1] == cluster2} \n",
    "\n",
    "    \n",
    "    # Check common neighbors\n",
    "    common_neighbors = neighbors_cluster1.intersection(neighbors_cluster2)\n",
    "\n",
    "    if len(common_neighbors) >= 2:\n",
    "        # merged nodes \n",
    "        merged_nodes = list(set(subtree1['nodes'] + subtree2['nodes']))\n",
    "\n",
    "        # Merge edges, renaming edges connected to cluster2 to clusters 1\n",
    "        merged_edges = subtree1['edges'] + \\\n",
    "                [(cluster1 if node == cluster2 else node,\n",
    "                 cluster1 if node2 == cluster2 else node2) \n",
    "                 for (node, node2) in subtree2['edges']]\n",
    "\n",
    "\n",
    "        # create merged subtrees\n",
    "        merged_subtree = {\n",
    "            'nodes': merged_nodes,\n",
    "            'edges': merged_edges,\n",
    "            'latent_clusters': [cluster1]\n",
    "        }\n",
    "\n",
    "        return merged_subtree, {}, True  # Return merged subtree and a flag indicating merging happened\n",
    "    \n",
    "    return subtree1, subtree2, False  # If no merging, return original subtrees and a flag indicating no merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.417300Z",
     "iopub.status.busy": "2024-03-02T21:32:17.417235Z",
     "iopub.status.idle": "2024-03-02T21:32:17.419629Z",
     "shell.execute_reply": "2024-03-02T21:32:17.419398Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_subtree_with_hidden_clusters_iterative(subtrees):\n",
    "    \"\"\"\n",
    "    This function iteratively merges subtrees based on the merging_criteria. \n",
    "    For each subtree, it will attempt to merge with every other subtree until no more merges are possible.\n",
    "    \"\"\"\n",
    "    merged_subtrees = {}\n",
    "    keys = list(subtrees.keys())\n",
    "    \n",
    "    while keys:\n",
    "        current_key = keys.pop(0)\n",
    "        current_subtree = subtrees[current_key]\n",
    "        \n",
    "        merged = True  # Assume a merge is possible to enter the while loop\n",
    "        while merged:\n",
    "            merged = False  # Reset the merged flag for each iteration  \n",
    "            # Check for a merge with every other subtree\n",
    "            for other_key in keys:\n",
    "                merged_subtree, _, did_merge = merging_criteria(current_subtree, subtrees[other_key])\n",
    "                if did_merge:\n",
    "                    print(\"current_clsuters: \" + ', ' .join(current_subtree['latent_clusters']))\n",
    "                    print(\"being merged cluster\" + ', ' .join(subtrees[other_key]['latent_clusters']))\n",
    "                    current_subtree = merged_subtree  # Update the current subtree to the merged one\n",
    "                    keys.remove(other_key)  # Remove the merged subtree from keys\n",
    "                    merged = True  # Mark that a merge has happened\n",
    "                    break  # Exit the loop and check if the merged subtree can be merged further\n",
    "\n",
    "        # After no more merges are possible, add the current subtree to the merged subtrees\n",
    "        merged_subtrees[current_key] = current_subtree\n",
    "\n",
    "    return merged_subtrees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.420890Z",
     "iopub.status.busy": "2024-03-02T21:32:17.420824Z",
     "iopub.status.idle": "2024-03-02T21:32:17.422394Z",
     "shell.execute_reply": "2024-03-02T21:32:17.422169Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_subtrees_with_hidden_nodes = merge_subtree_with_hidden_clusters_iterative(subtree_with_hidden_clusters)\n",
    "merged_subtrees_without_hidden_clusters = merged_subtrees_without_hidden(subtree_without_hidden_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.423448Z",
     "iopub.status.busy": "2024-03-02T21:32:17.423383Z",
     "iopub.status.idle": "2024-03-02T21:32:17.425425Z",
     "shell.execute_reply": "2024-03-02T21:32:17.425225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge the subtrees above first, and then we can dectect cycles and start deleting edges\n",
    "combined_tree = {\n",
    "    \"nodes\": merged_subtrees_without_hidden_clusters[\"nodes\"],\n",
    "    \"edges\": merged_subtrees_without_hidden_clusters[\"edges\"],\n",
    "    \"latent_clusters\": merged_subtrees_without_hidden_clusters[\"latent_clusters\"]\n",
    "}\n",
    "\n",
    "# Iterate over each key in merged_subtrees_with_hidden_nodes\n",
    "for key in merged_subtrees_with_hidden_nodes:\n",
    "    # Combine nodes\n",
    "    combined_tree[\"nodes\"].extend(merged_subtrees_with_hidden_nodes[key][\"nodes\"])\n",
    "    # Ensure there are no duplicate nodes\n",
    "    combined_tree[\"nodes\"] = list(set(combined_tree[\"nodes\"]))\n",
    "\n",
    "    # Combine edges\n",
    "    combined_tree[\"edges\"].extend(merged_subtrees_with_hidden_nodes[key][\"edges\"])\n",
    "    combined_tree[\"edges\"] = list(set(combined_tree[\"edges\"]))\n",
    "\n",
    "    # Combine latent_clusters\n",
    "    combined_tree[\"latent_clusters\"].extend(merged_subtrees_with_hidden_nodes[key][\"latent_clusters\"])\n",
    "    # Ensure there are no duplicate latent_clusters\n",
    "    combined_tree[\"latent_clusters\"] = list(set(combined_tree[\"latent_clusters\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.426620Z",
     "iopub.status.busy": "2024-03-02T21:32:17.426554Z",
     "iopub.status.idle": "2024-03-02T21:32:17.427984Z",
     "shell.execute_reply": "2024-03-02T21:32:17.427768Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_tree[\"nodes\"] += combined_tree[\"latent_clusters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.429138Z",
     "iopub.status.busy": "2024-03-02T21:32:17.429076Z",
     "iopub.status.idle": "2024-03-02T21:32:17.430771Z",
     "shell.execute_reply": "2024-03-02T21:32:17.430574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing duplicate and reversed edges from combined_tree\n",
    "unique_edges = set()\n",
    "cleaned_edges = []\n",
    "\n",
    "for edge in combined_tree['edges']:\n",
    "    if edge not in unique_edges and (edge[1], edge[0]) not in unique_edges:\n",
    "        cleaned_edges.append(edge)\n",
    "        unique_edges.add(edge)\n",
    "\n",
    "combined_tree['edges'] = cleaned_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.431933Z",
     "iopub.status.busy": "2024-03-02T21:32:17.431852Z",
     "iopub.status.idle": "2024-03-02T21:32:17.480528Z",
     "shell.execute_reply": "2024-03-02T21:32:17.480303Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_subtree(combined_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now delete cycles in a good way\n",
    "- we clean the edges first, no duplicates\n",
    "- detect the cycle, only compare the correlation among pairs of visible nodes \n",
    "- delete the one has the smallest absolute value of correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.481902Z",
     "iopub.status.busy": "2024-03-02T21:32:17.481813Z",
     "iopub.status.idle": "2024-03-02T21:32:17.484751Z",
     "shell.execute_reply": "2024-03-02T21:32:17.484544Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_cycle(graoh_with_cycles):\n",
    "\n",
    "    # Extract nodes, edges, and latent_clusters from the combined_tree\n",
    "    nodes = graoh_with_cycles['nodes']\n",
    "    edges = graoh_with_cycles['edges']\n",
    "    latent_clusters = graoh_with_cycles['latent_clusters']\n",
    "\n",
    "    # Initialize a default dictionary to hold the adjacency list representation of the graph.\n",
    "    graph = defaultdict(list)\n",
    "    \n",
    "    # Get the number of nodes in the graph.\n",
    "    V = len(nodes)\n",
    "    \n",
    "    # Create a dictionary to map node names to indices for easier access.\n",
    "    node_indices = {node: i for i, node in enumerate(nodes)}\n",
    "    \n",
    "    # Initialize a list to keep track of visited nodes during DFS.\n",
    "    visited = [False] * V\n",
    "    \n",
    "    # Construct the graph from the given edges.\n",
    "    # For an undirected graph, add the edge in both directions.\n",
    "    for u, v in edges:\n",
    "        graph[u].append(v)\n",
    "        graph[v].append(u)\n",
    "        \n",
    "    def dfs(v, visited, parent, path):\n",
    "        # Mark the current node as visited and add it to the current path.\n",
    "        visited[node_indices[v]] = True\n",
    "        path.append(v)\n",
    "        \n",
    "        for i in graph[v]:\n",
    "            if not visited[node_indices[i]]:\n",
    "                if dfs(i, visited, v, path):\n",
    "                    return True\n",
    "            elif i != parent:  # A cycle is detected.\n",
    "                # Capture the cycle by tracing back from the current node to the first occurrence of 'i'.\n",
    "                cycle_start_idx = path.index(i)\n",
    "                path.append(i)\n",
    "                del path[:cycle_start_idx]\n",
    "                return True\n",
    "                \n",
    "        # Backtrack\n",
    "        path.pop()\n",
    "        return False\n",
    "    \n",
    "    for node in nodes:\n",
    "        if not visited[node_indices[node]]:\n",
    "            path = []\n",
    "            if dfs(node, visited, None, path):\n",
    "                return True, path\n",
    "                \n",
    "    return False, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.485945Z",
     "iopub.status.busy": "2024-03-02T21:32:17.485865Z",
     "iopub.status.idle": "2024-03-02T21:32:17.487398Z",
     "shell.execute_reply": "2024-03-02T21:32:17.487204Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_index(node_name, latent_clusters):\n",
    "    # Extract index from node name; modify this based on your actual node naming convention.\n",
    "    return int(node_name[2:]) if node_name in latent_clusters else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.488483Z",
     "iopub.status.busy": "2024-03-02T21:32:17.488421Z",
     "iopub.status.idle": "2024-03-02T21:32:17.491136Z",
     "shell.execute_reply": "2024-03-02T21:32:17.490741Z"
    }
   },
   "outputs": [],
   "source": [
    "def edge_to_remove_from_cycle(cycle, latent_clusters,correlation_matrix):\n",
    "    cycle_nodes = cycle[1]\n",
    "\n",
    "     # Filter out edges that involve nodes in the latent_clusters.\n",
    "    valid_edges = [(cycle_nodes[i], cycle_nodes[i + 1]) for i in range(len(cycle_nodes) - 1) \n",
    "                    if cycle_nodes[i] not in latent_clusters and cycle_nodes[i + 1] not in latent_clusters]\n",
    "\n",
    "    # if there are valid edges\n",
    "    if valid_edges:\n",
    "        correlations = [abs(correlation_matrix.loc[edge[0], edge[1]]) for edge in valid_edges]\n",
    "        # Identify the edge with the smallest absolute correlation.\n",
    "        min_index = correlations.index(min(correlations))\n",
    "        return valid_edges[min_index]\n",
    "\n",
    "    \n",
    "    else:\n",
    "        hidden_edges = [(cycle_nodes[i], cycle_nodes[i + 1]) for i in range(len(cycle_nodes) - 1) \n",
    "                    if cycle_nodes[i] in latent_clusters or cycle_nodes[i + 1] in latent_clusters]\n",
    "        \n",
    "        # Assuming hidden node names are in a format where the index can be extracted and compared.\n",
    "        max_hidden_node_edge = max(\n",
    "            hidden_edges, \n",
    "            key=lambda edge: max(get_index(edge[0], latent_clusters), \n",
    "            get_index(edge[1], latent_clusters))\n",
    "            )\n",
    "        return max_hidden_node_edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.492265Z",
     "iopub.status.busy": "2024-03-02T21:32:17.492204Z",
     "iopub.status.idle": "2024-03-02T21:32:17.494267Z",
     "shell.execute_reply": "2024-03-02T21:32:17.494068Z"
    }
   },
   "outputs": [],
   "source": [
    "def cycle_removal(graph, original_correlation_matrix):\n",
    "    while True:\n",
    "        cycle = detect_cycle(graph)\n",
    "        print(cycle)\n",
    "        \n",
    "        if not cycle[0]:# If no cycle detected\n",
    "            break\n",
    "        edge_to_delete = edge_to_remove_from_cycle(cycle, graph['latent_clusters'], original_correlation_matrix)\n",
    "        \n",
    "        if edge_to_delete in graph['edges']:\n",
    "            graph['edges'].remove(edge_to_delete)\n",
    "        elif (edge_to_delete[1], edge_to_delete[0]) in graph['edges']:\n",
    "            graph['edges'].remove((edge_to_delete[1], edge_to_delete[0]))  \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.495502Z",
     "iopub.status.busy": "2024-03-02T21:32:17.495435Z",
     "iopub.status.idle": "2024-03-02T21:32:17.497384Z",
     "shell.execute_reply": "2024-03-02T21:32:17.497140Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_tree = cycle_removal(combined_tree, original_correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.498618Z",
     "iopub.status.busy": "2024-03-02T21:32:17.498557Z",
     "iopub.status.idle": "2024-03-02T21:32:17.547870Z",
     "shell.execute_reply": "2024-03-02T21:32:17.547659Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_subtree(updated_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.549093Z",
     "iopub.status.busy": "2024-03-02T21:32:17.549026Z",
     "iopub.status.idle": "2024-03-02T21:32:17.550608Z",
     "shell.execute_reply": "2024-03-02T21:32:17.550409Z"
    }
   },
   "outputs": [],
   "source": [
    "task3_output = updated_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- update the output of task 2 so it has updated clusters\n",
    "- because task4 initially only consider the clusters after the merge of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.551761Z",
     "iopub.status.busy": "2024-03-02T21:32:17.551699Z",
     "iopub.status.idle": "2024-03-02T21:32:17.553895Z",
     "shell.execute_reply": "2024-03-02T21:32:17.553690Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_latent_cluster(subtrees_dic_with_latent_clusters):\n",
    "    \"\"\"\n",
    "    Removes latent_clusters and edges associated with them from the subtrees.\n",
    "    \n",
    "    Args:\n",
    "    - collapsed_subtrees (dict): A dictionary representing the subtrees.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A new dictionary with latent_clusters removed and corresponding edges filtered out.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a new dictionary to store the modified data\n",
    "    input_task4_from_task2 = {}\n",
    "\n",
    "    # iterate through each subtree in the dictionary\n",
    "    for key, value in subtrees_dic_with_latent_clusters.items():\n",
    "        # create a deep copy of the current subtree to ensure we don't modify the original data\n",
    "        new_value = value.copy()\n",
    "        \n",
    "        # empty the latent_clusters list\n",
    "        new_value['latent_clusters'] = []\n",
    "        \n",
    "        # filter out edges that contain any latent_cluster\n",
    "        new_value['edges'] = [\n",
    "            edge for edge in value['edges'] \n",
    "            if not any(node in value['latent_clusters'] for node in edge)\n",
    "        ]\n",
    "        \n",
    "        # add the modified subtree to the new dictionary\n",
    "        input_task4_from_task2[key] = new_value\n",
    "\n",
    "    return input_task4_from_task2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.555022Z",
     "iopub.status.busy": "2024-03-02T21:32:17.554957Z",
     "iopub.status.idle": "2024-03-02T21:32:17.557835Z",
     "shell.execute_reply": "2024-03-02T21:32:17.557645Z"
    }
   },
   "outputs": [],
   "source": [
    "def hidden_root_detection (V_tilda, r):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to perform hidden root detection based on V_tilda and a current key.\n",
    "\n",
    "    :param V_tilda: Dictionary containing V_tilda_i as keys and their corresponding sets.\n",
    "    :param current_key: Current subtree iteration, e.g., 'T_1', 'T_2', etc.\n",
    "    \n",
    "    :return: Returns two dictionaries, left_relative_complement and right_relative_complement.\n",
    "    \"\"\"\n",
    "    # Initialize the result dictionaries \n",
    "    left_relative_complement  = {}\n",
    "    right_relative_complement  = {}\n",
    "\n",
    "    # Calculate the relative complements \n",
    "    V_tilda_r = V_tilda[f'V_tilda_{r}']\n",
    "    for key, value in V_tilda.items():\n",
    "        i = int(key.split('_')[2])\n",
    "        if i != r:\n",
    "            left_difference = V_tilda_r - value\n",
    "            right_difference = value - V_tilda_r\n",
    "        \n",
    "            left_length = 0 if not left_difference else len(left_difference)\n",
    "            right_length = 0 if not right_difference else len(right_difference)\n",
    "        \n",
    "            left_relative_complement[f'relative_complement_{r}_{i}'] = left_length\n",
    "            right_relative_complement[f'relative_complement_{i}_{r}'] = right_length\n",
    "\n",
    "    # check conditions\n",
    "    all_conditions_met = True\n",
    "    \n",
    "    for key, value in left_relative_complement.items():\n",
    "        # print(key)\n",
    "        i = int(key.split('_')[3])\n",
    "        # print(i)\n",
    "\n",
    "        if i != r:\n",
    "            left_key =  f'relative_complement_{r}_{i}'\n",
    "            right_key = f'relative_complement_{i}_{r}'\n",
    "\n",
    "            left_value = left_relative_complement[left_key]\n",
    "            # print(left_value)\n",
    "            right_value = right_relative_complement[right_key]\n",
    "            # print(right_value)\n",
    "        \n",
    "            # check \n",
    "            if (left_value <= 1 and right_value >1):\n",
    "                # print(f\"Condition not met for: {key}, left value: {value}, right value: {right_value}\")  # Print the issue\n",
    "                return False\n",
    "            \n",
    "    if all_conditions_met:\n",
    "            return True, r, left_relative_complement, right_relative_complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.559008Z",
     "iopub.status.busy": "2024-03-02T21:32:17.558945Z",
     "iopub.status.idle": "2024-03-02T21:32:17.562449Z",
     "shell.execute_reply": "2024-03-02T21:32:17.562260Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemma_18(r, left_relative_complement, right_relative_complement, V_tilda, subtree):\n",
    "    \n",
    "    big_I = {r}\n",
    "    big_I_copy = {r}\n",
    "\n",
    "    for key in left_relative_complement:\n",
    "        i = int(key.split('_')[3])\n",
    "\n",
    "        if i!= r:\n",
    "            left_key =  f'relative_complement_{r}_{i}'\n",
    "            right_key = f'relative_complement_{i}_{r}'\n",
    "\n",
    "            left_value = left_relative_complement[left_key]\n",
    "            # print(left_value)\n",
    "            right_value = right_relative_complement[right_key]\n",
    "            # print(right_value)\n",
    "\n",
    "            if left_value == 1 and right_value == 1:\n",
    "                big_I.add(i)\n",
    "\n",
    "    for key in left_relative_complement:\n",
    "        i = int(key.split('_')[3])\n",
    "\n",
    "        if i!= r:\n",
    "            left_key =  f'relative_complement_{r}_{i}'\n",
    "            right_key = f'relative_complement_{i}_{r}'\n",
    "\n",
    "            left_value = left_relative_complement[left_key]\n",
    "            # print(left_value)\n",
    "            right_value = right_relative_complement[right_key]\n",
    "            # print(right_value)\n",
    "\n",
    "            if left_value ==1  and right_value in (0,1):\n",
    "                big_I_copy.add(i)\n",
    "            \n",
    "\n",
    "    # Compute big_W and big_Wbar\n",
    "    big_W = set().union(*[V_tilda[f'V_tilda_{i}'] for i in big_I])\n",
    "    big_Wbar = set().union(*[V_tilda[i] for i in V_tilda.keys() if int(i.split('_')[2]) not in big_I])\n",
    "    intersection_W = big_W & big_Wbar\n",
    "\n",
    "    # Calculating nodes_linked_to_hidden\n",
    "    nodes_linked_to_hidden = big_W - big_Wbar\n",
    "\n",
    "    # Need a second choice of nodes_linked_to_hidden\n",
    "    big_W_copy = set().union(*[V_tilda[f'V_tilda_{i}'] for i in big_I_copy])\n",
    "    big_Wbar_copy = set().union(*[V_tilda[i] for i in V_tilda.keys() if int(i.split('_')[2]) not in big_I_copy])\n",
    "    intersection_W_copy = big_W_copy & big_Wbar_copy\n",
    "\n",
    "\n",
    "    if not nodes_linked_to_hidden: \n",
    "        # print(\"loose the condition\")\n",
    "        differerence = big_W_copy - big_Wbar_copy\n",
    "        if differerence:\n",
    "            nodes_linked_to_hidden = {list(differerence)[0]}\n",
    "            print(nodes_linked_to_hidden)\n",
    "            return nodes_linked_to_hidden, intersection_W_copy,big_I_copy\n",
    "        else:\n",
    "            # print(\"loose the condition again\")\n",
    "            return False\n",
    "        \n",
    "    return nodes_linked_to_hidden, intersection_W, big_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.563540Z",
     "iopub.status.busy": "2024-03-02T21:32:17.563472Z",
     "iopub.status.idle": "2024-03-02T21:32:17.565612Z",
     "shell.execute_reply": "2024-03-02T21:32:17.565414Z"
    }
   },
   "outputs": [],
   "source": [
    "def task_4b(intersection_W, big_I, subtrees, inter_count):\n",
    "    # print('intersection_W:')\n",
    "    # print(intersection_W)\n",
    "    # reorder and deduplicate intersection_W\n",
    "    intersection_W = sorted(set(intersection_W), key=lambda x: int(x[1:]))\n",
    "\n",
    "    # initialze n_k\n",
    "    n_k = len(intersection_W)\n",
    "\n",
    "    # create n_k number of hidden clusters and add edges \n",
    "    for i in big_I:\n",
    "        for k in range(n_k):\n",
    "            new_cluster =  f\"C_{inter_count}_{i}_{k+1}\"\n",
    "            new_edge =  (new_cluster, intersection_W[k])\n",
    "            subtree_key = f\"T_{i}\"\n",
    "            subtrees[subtree_key]['latent_clusters'].append(new_cluster)\n",
    "            subtrees[subtree_key]['edges'].append(new_edge)\n",
    "    return subtrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.566766Z",
     "iopub.status.busy": "2024-03-02T21:32:17.566703Z",
     "iopub.status.idle": "2024-03-02T21:32:17.568823Z",
     "shell.execute_reply": "2024-03-02T21:32:17.568630Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_hidden_cluster_merging_criteria(cluster1, cluster2, V_tilda_to_compare,subtree):\n",
    "    # Determine the visible neighbors of each cluster within the subtree\n",
    "    visible_neighbors_cluster1 = [\n",
    "        target for node, target in subtree['edges'] \n",
    "        if node == cluster1 and target in subtree['nodes']\n",
    "        ]\n",
    "    visible_neighbors_cluster2 = [\n",
    "        target for node, target in subtree['edges'] \n",
    "        if node == cluster2 and target in subtree['nodes']\n",
    "        ]\n",
    "\n",
    "    # compute the union of the visible neighbors\n",
    "    visible_neighbors = set (visible_neighbors_cluster1 + visible_neighbors_cluster2)\n",
    "\n",
    "    # check the overlap with the nodes iin V_tilda_to_compare \n",
    "    distinct_nodes_count = sum(1 for V_tilda_nodes in V_tilda_to_compare.values()if len(V_tilda_nodes.intersection(visible_neighbors)) > 0)\n",
    "\n",
    "    # return True if there are at least 2 distinct nodes, otherwise return False \n",
    "    return distinct_nodes_count >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.569933Z",
     "iopub.status.busy": "2024-03-02T21:32:17.569854Z",
     "iopub.status.idle": "2024-03-02T21:32:17.572659Z",
     "shell.execute_reply": "2024-03-02T21:32:17.572461Z"
    }
   },
   "outputs": [],
   "source": [
    "def task_4c(subtree_from_4b, big_I, V_tilda):\n",
    "    V_tilda_to_compare = {k: v for k, v in V_tilda.items() if int(k.split('_')[-1]) not in big_I}\n",
    "    \n",
    "    # Update subtrees based on big_I\n",
    "    for i in big_I:\n",
    "        subtree_key = f\"T_{i}\"\n",
    "        subtree = subtree_from_4b[subtree_key] \n",
    "        latent_clusters = subtree['latent_clusters']\n",
    "\n",
    "        while latent_clusters:\n",
    "            any_merge_happened =  False # This flag checks if any merge happended in the entire iteration\n",
    "            current_cluster = latent_clusters[0]\n",
    "            merged = True # assume a merger is possible to enter the while loop \n",
    "\n",
    "            while merged:\n",
    "                merged = False # reset the mergerd flag for each iteration\n",
    "                for other_cluster in [oc for oc in latent_clusters if oc != current_cluster]:\n",
    "                    if new_hidden_cluster_merging_criteria(current_cluster, other_cluster, V_tilda_to_compare, subtree):\n",
    "                        latent_clusters.remove(other_cluster) # remove the merged cluster from latent_clusters\n",
    "                        # modify the edges by using the smaller index's latent cluster \n",
    "                        subtree['edges'] = [\n",
    "                            (current_cluster if node == other_cluster else node, target) \n",
    "                            for node, target in subtree['edges']\n",
    "                            ]\n",
    "                        merged = True # mark that a merge has happened \n",
    "                        any_merge_happened = True\n",
    "                        break # exit the loop and check if the merged cluster can be merged clusters \n",
    "            if not any_merge_happened:\n",
    "                break\n",
    "                        \n",
    "            # after no more merges are possible, add the current cluster to the merged clusters    \n",
    "        subtree['latent_clusters'] =  latent_clusters\n",
    "        subtree_from_4b[subtree_key] = subtree # Store the updated subtree in the resulting dictionary\n",
    "\n",
    "    return subtree_from_4b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.573805Z",
     "iopub.status.busy": "2024-03-02T21:32:17.573740Z",
     "iopub.status.idle": "2024-03-02T21:32:17.577536Z",
     "shell.execute_reply": "2024-03-02T21:32:17.577340Z"
    }
   },
   "outputs": [],
   "source": [
    "def HMCA(current_tree_key, subtrees, neighbors_of_current_clusters, current_hidden_cluster,inter_count):\n",
    "    current_key = current_tree_key\n",
    "    r = int(current_key.split('_')[1])\n",
    "\n",
    "    # Initialize V_i for each subtree T_i\n",
    "    V = {key: set(subtree['nodes']) for key, subtree in subtrees.items()}\n",
    "\n",
    "    # Compute V~i for each subtree T_i\n",
    "    V_tilda = {f'V_tilda_{key.split(\"_\")[1]}': nodes.intersection(neighbors_of_current_clusters) \n",
    "           for key, nodes in V.items()}  \n",
    "\n",
    "    independence_relation_key = f'independence_{r}'\n",
    "    independence_relation = set()\n",
    "\n",
    "    if not V_tilda.get(f'V_tilda_{r}'):   # Check if V_tilda_r is empty\n",
    "        return subtrees, {independence_relation_key: independence_relation}\n",
    "    \n",
    "    if all (not v for v in V_tilda.values()):  # Step 2, Check if all V~i are empty \n",
    "        return subtrees, {independence_relation_key: independence_relation}\n",
    "    \n",
    "    # only need those that has intersection not empty\n",
    "    V_tilda = {key: value for key, value in V_tilda.items() if value}\n",
    "\n",
    "    # Call hidden_root  \n",
    "    result = hidden_root_detection(V_tilda, r)\n",
    "\n",
    "    if result:\n",
    "        _, r, left_relative_complement, right_relative_complement = result\n",
    "        lemma_result= lemma_18(r, left_relative_complement, right_relative_complement, V_tilda, subtrees[current_key])\n",
    "\n",
    "        if not lemma_result:\n",
    "            # print(\"lemma 18 returned False. Need to debug\")\n",
    "            return subtrees,{independence_relation_key: independence_relation}\n",
    "\n",
    "        nodes_linked_to_hidden, intersection_W, big_I = lemma_result\n",
    "\n",
    "        if nodes_linked_to_hidden:\n",
    "            new_node = f'y_h_{inter_count}_{r}' # create the new node \n",
    "            # create edges between the new node and each node in nodes_linked_to_hidden\n",
    "            new_edges = [(new_node, node) for node in nodes_linked_to_hidden] \n",
    "\n",
    "            # add this node and edge to all T_i where in big_I\n",
    "            for i in big_I:\n",
    "                key = f'T_{i}'\n",
    "                subtrees[key]['nodes'].append(new_node)\n",
    "                subtrees[key]['edges'].extend(new_edges)\n",
    "\n",
    "            # Create independence relations\n",
    "            for y in subtrees[key]['nodes']:\n",
    "                if y != new_node:\n",
    "                    independence_relation.add((new_node, y))    \n",
    "                    \n",
    "            if len(intersection_W) > 0:\n",
    "                subtrees_4b = task_4b(intersection_W, big_I, subtrees,inter_count)\n",
    "                subtrees_4c = task_4c(subtrees_4b, big_I, V_tilda)\n",
    "                return subtrees_4c, {independence_relation_key: independence_relation}\n",
    "            else:\n",
    "                return subtrees,{independence_relation_key: independence_relation}\n",
    "        else:\n",
    "            print(\"linked visible nodes can not be found, need to debug where went wrong\")\n",
    "            return subtrees,{independence_relation_key: independence_relation}\n",
    "    else:\n",
    "        return subtrees,{independence_relation_key: independence_relation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.578664Z",
     "iopub.status.busy": "2024-03-02T21:32:17.578604Z",
     "iopub.status.idle": "2024-03-02T21:32:17.580537Z",
     "shell.execute_reply": "2024-03-02T21:32:17.580344Z"
    }
   },
   "outputs": [],
   "source": [
    "# a function do the categorize during task 4, we refer the one task3, but not exactly the same\n",
    "def categorizing_subtrees_task4(subtrees):\n",
    "    subtree_with_hidden_clusters = {}\n",
    "    subtree_without_hidden_clusters = {}\n",
    "\n",
    "    for key, subtree in subtrees.items():\n",
    "        if subtree['latent_clusters']:\n",
    "            subtree_with_hidden_clusters[key] = subtree\n",
    "            # Remove latent clusters from the nodes list\n",
    "            for cluster in subtree['latent_clusters']:\n",
    "                if cluster in subtree['nodes']:\n",
    "                    subtree['nodes'].remove(cluster)\n",
    "        else:\n",
    "            subtree_without_hidden_clusters[key] = subtree\n",
    "            \n",
    "    return subtree_with_hidden_clusters, subtree_without_hidden_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.581668Z",
     "iopub.status.busy": "2024-03-02T21:32:17.581604Z",
     "iopub.status.idle": "2024-03-02T21:32:17.584333Z",
     "shell.execute_reply": "2024-03-02T21:32:17.584147Z"
    }
   },
   "outputs": [],
   "source": [
    "def task_3_for_task4(subtrees):\n",
    "\n",
    "    subtree_with_hidden_clusters, subtree_without_hidden_clusters = categorizing_subtrees_task4(subtrees)\n",
    "    merged_subtrees_with_hidden_nodes = merge_subtree_with_hidden_clusters_iterative(subtree_with_hidden_clusters)\n",
    "    merged_subtrees_without_hidden_clusters = merged_subtrees_without_hidden(subtree_without_hidden_clusters)\n",
    "    # Merge the subtrees above first, and then we can dectect cycles and start deleting edges\n",
    "    combined_tree = {\n",
    "        \"nodes\": merged_subtrees_without_hidden_clusters[\"nodes\"],\n",
    "        \"edges\": merged_subtrees_without_hidden_clusters[\"edges\"],\n",
    "        \"latent_clusters\": merged_subtrees_without_hidden_clusters[\"latent_clusters\"]\n",
    "    }\n",
    "\n",
    "    # Iterate over each key in merged_subtrees_with_hidden_nodes\n",
    "    for key in merged_subtrees_with_hidden_nodes:\n",
    "        # Combine nodes\n",
    "        combined_tree[\"nodes\"].extend(merged_subtrees_with_hidden_nodes[key][\"nodes\"])\n",
    "        # Ensure there are no duplicate nodes\n",
    "        combined_tree[\"nodes\"] = list(set(combined_tree[\"nodes\"]))\n",
    "\n",
    "        # Combine edges\n",
    "        combined_tree[\"edges\"].extend(merged_subtrees_with_hidden_nodes[key][\"edges\"])\n",
    "        combined_tree[\"edges\"] = list(set(combined_tree[\"edges\"]))\n",
    "\n",
    "        # Combine latent_clusters\n",
    "        combined_tree[\"latent_clusters\"].extend(merged_subtrees_with_hidden_nodes[key][\"latent_clusters\"])\n",
    "        # Ensure there are no duplicate latent_clusters\n",
    "        combined_tree[\"latent_clusters\"] = list(set(combined_tree[\"latent_clusters\"]))\n",
    "\n",
    "        unique_edges = set()\n",
    "        cleaned_edges = []\n",
    "\n",
    "        for edge in combined_tree['edges']:\n",
    "            if edge not in unique_edges and (edge[1], edge[0]) not in unique_edges:\n",
    "                cleaned_edges.append(edge)\n",
    "                unique_edges.add(edge)\n",
    "\n",
    "        combined_tree['edges'] = cleaned_edges\n",
    "\n",
    "    return combined_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.585429Z",
     "iopub.status.busy": "2024-03-02T21:32:17.585365Z",
     "iopub.status.idle": "2024-03-02T21:32:17.587055Z",
     "shell.execute_reply": "2024-03-02T21:32:17.586843Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_neighbors_of_a_latent_clusters(quasi_skeleton, current_hidden_cluster):\n",
    "\n",
    "    neighbors_of_current_clusters = set()\n",
    "    for edge in quasi_skeleton['edges']:\n",
    "        if current_hidden_cluster in edge: \n",
    "            neighbors_of_current_clusters.add(edge[0] if edge[1] == current_hidden_cluster else edge[1])\n",
    "    \n",
    "    return neighbors_of_current_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.588183Z",
     "iopub.status.busy": "2024-03-02T21:32:17.588096Z",
     "iopub.status.idle": "2024-03-02T21:32:17.590669Z",
     "shell.execute_reply": "2024-03-02T21:32:17.590460Z"
    }
   },
   "outputs": [],
   "source": [
    "def task4(task2_subtree, task3_output):\n",
    "    \n",
    "    subtrees = task2_subtree.copy()\n",
    "    quasi_skeleton = task3_output.copy()\n",
    "    # create a dictionary to store indpendence_relations for each interartion \n",
    "    independence_relations_dict = {}\n",
    "\n",
    "    # Step 1: initialize P\n",
    "    P = set(quasi_skeleton['latent_clusters'])\n",
    "    inter_count = 1 \n",
    "    max_inter = 20\n",
    "\n",
    "    # Step 2: Loop until P is empty\n",
    "    while P:\n",
    "\n",
    "        print(inter_count)\n",
    "        print(P)\n",
    "        \n",
    "        # create a new dictionary for the current inter_count\n",
    "        independence_relations_dict[inter_count] = {}\n",
    "\n",
    "        current_hidden_cluster = list(P)[0] # taking the first element of P\n",
    "        subtrees = remove_latent_cluster(subtrees)\n",
    "        neighbors_of_current_clusters = find_neighbors_of_a_latent_clusters(quasi_skeleton,current_hidden_cluster)\n",
    "\n",
    "        # For each subtree in subtree \n",
    "        for T_i in list(subtrees.keys()): # iterating over a copy of the keys to avoid dictionarsize change during iteration \n",
    "            # run HMCA on the subtree \n",
    "\n",
    "            updated_subtrees, new_independence_relations = HMCA(\n",
    "                T_i, \n",
    "                subtrees, \n",
    "                neighbors_of_current_clusters, \n",
    "                current_hidden_cluster, \n",
    "                inter_count)\n",
    "            \n",
    "            # update subtrees with the output from HMCA\n",
    "            subtrees = updated_subtrees\n",
    "\n",
    "            # accumulate the new independence relations \n",
    "            independence_relations_dict[inter_count][f\"independence_relation_{T_i}\"] = new_independence_relations\n",
    "\n",
    "        quasi_skeleton = task_3_for_task4(subtrees)\n",
    "        print(quasi_skeleton['latent_clusters'])\n",
    "        P = set(quasi_skeleton['latent_clusters'])\n",
    "        # now we should update t\n",
    "\n",
    "        # keep checking the count \n",
    "        inter_count  +=1\n",
    "        if inter_count >= max_inter:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Step 3: Return the updated task2_subtree\n",
    "    return subtrees, independence_relations_dict, quasi_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.591728Z",
     "iopub.status.busy": "2024-03-02T21:32:17.591667Z",
     "iopub.status.idle": "2024-03-02T21:32:17.593490Z",
     "shell.execute_reply": "2024-03-02T21:32:17.593243Z"
    }
   },
   "outputs": [],
   "source": [
    "task4_subtrees, task4_independence_relations, task4_skeleton  = task4(collapsed_subtrees,task3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.594538Z",
     "iopub.status.busy": "2024-03-02T21:32:17.594456Z",
     "iopub.status.idle": "2024-03-02T21:32:17.596620Z",
     "shell.execute_reply": "2024-03-02T21:32:17.596407Z"
    }
   },
   "outputs": [],
   "source": [
    "task4_independence_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.597752Z",
     "iopub.status.busy": "2024-03-02T21:32:17.597679Z",
     "iopub.status.idle": "2024-03-02T21:32:17.599412Z",
     "shell.execute_reply": "2024-03-02T21:32:17.599218Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_cycle(graph_with_cycles):\n",
    "    # Extract nodes and edges from the graph\n",
    "    nodes = graph_with_cycles['nodes']\n",
    "    edges = graph_with_cycles['edges']\n",
    "\n",
    "    # Construct a graph using networkx\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    # Use networkx's simple_cycles function to find cycles\n",
    "    # It returns a list of cycles, where each cycle is a list of nodes\n",
    "    cycles = list(nx.simple_cycles(G))\n",
    "    \n",
    "    if cycles:  # If there's at least one cycle\n",
    "        return True, cycles[0]  # Return the first detected cycle\n",
    "    else:\n",
    "        return False, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.600484Z",
     "iopub.status.busy": "2024-03-02T21:32:17.600423Z",
     "iopub.status.idle": "2024-03-02T21:32:17.603761Z",
     "shell.execute_reply": "2024-03-02T21:32:17.603552Z"
    }
   },
   "outputs": [],
   "source": [
    "task4_skeleton_cleaned = cycle_removal(task4_skeleton, original_correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.604943Z",
     "iopub.status.busy": "2024-03-02T21:32:17.604865Z",
     "iopub.status.idle": "2024-03-02T21:32:17.652454Z",
     "shell.execute_reply": "2024-03-02T21:32:17.652231Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_subtree(task4_skeleton_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Obtain the pattern of the latent polytree from task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expand the correlatin matrix from task 1\n",
    "- make a copy first\n",
    "- if the pair occurs in the indepence relation we gain from task 4, put 1 in the entry, 0 otherwise \n",
    "- make sure all entries get populated by usinmg symmetry of correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.653806Z",
     "iopub.status.busy": "2024-03-02T21:32:17.653721Z",
     "iopub.status.idle": "2024-03-02T21:32:17.656344Z",
     "shell.execute_reply": "2024-03-02T21:32:17.656142Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a copy of the original matrix\n",
    "expanded_matrix = new_correlation_matrix.copy()\n",
    "\n",
    "# Extract hidden nodes from task4_independence_relations\n",
    "hidden_nodes = set()\n",
    "for iteration_relations in task4_independence_relations.values():\n",
    "    for relations in iteration_relations.values():\n",
    "        for relation in relations.values():\n",
    "            for pair in relation:\n",
    "                for node in pair:\n",
    "                    if node in pair:\n",
    "                        if node.startswith('y_h') and node not in hidden_nodes:\n",
    "                            hidden_nodes.add(node)\n",
    "                            \n",
    "# Expand expanded_matrix based on the hidden nodes\n",
    "for hidden_node in hidden_nodes:\n",
    "    expanded_matrix[hidden_node] = 0 #add a new column \n",
    "    expanded_matrix.loc[hidden_node] = 0 #add a new row \n",
    "\n",
    "\n",
    "# Fill up the entries of the expanded correlation matrix based on task4_independence_relations\n",
    "for iteration_relations in task4_independence_relations.values():\n",
    "    for relations in iteration_relations.values():\n",
    "        for relation in relations.values():\n",
    "            for pair in relation:\n",
    "                expanded_matrix.loc[pair[0], pair[1]] = 1\n",
    "                expanded_matrix.loc[pair[1], pair[0]] = 1\n",
    "\n",
    "# Ensure the diagonal values are 1\n",
    "np.fill_diagonal(expanded_matrix.values, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rename the hidden nodes by keep counting forward for better indexing\n",
    "- update task4_skeleton_cleaned['nodes'] \n",
    "- update task4_skeleton_cleaned['edges'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.657523Z",
     "iopub.status.busy": "2024-03-02T21:32:17.657448Z",
     "iopub.status.idle": "2024-03-02T21:32:17.659400Z",
     "shell.execute_reply": "2024-03-02T21:32:17.659180Z"
    }
   },
   "outputs": [],
   "source": [
    "def rename_hidden_nodes(nodes):\n",
    "    # Find the largest visible node number\n",
    "    max_visible_node = max(int(node[1:]) for node in nodes if not node.startswith('y_h'))\n",
    "    \n",
    "    # Create a mapping for renaming and a set for renamed hidden nodes\n",
    "    rename_map = {}\n",
    "    renamed_hidden_nodes = set()\n",
    "    hidden_index = 1\n",
    "    for node in nodes:\n",
    "        if node.startswith('y_h'):\n",
    "            new_name = f'y{max_visible_node + hidden_index}'\n",
    "            rename_map[node] = new_name\n",
    "            renamed_hidden_nodes.add(new_name)\n",
    "            hidden_index += 1\n",
    "            \n",
    "    # Rename nodes\n",
    "    renamed_nodes = [rename_map.get(node, node) for node in nodes]\n",
    "    return renamed_nodes, rename_map, renamed_hidden_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.660484Z",
     "iopub.status.busy": "2024-03-02T21:32:17.660407Z",
     "iopub.status.idle": "2024-03-02T21:32:17.662125Z",
     "shell.execute_reply": "2024-03-02T21:32:17.661942Z"
    }
   },
   "outputs": [],
   "source": [
    "# get input for rename_hidden_nodes function \n",
    "nodes = task4_skeleton_cleaned['nodes']\n",
    "\n",
    "# retrive renamed_nodes/map/edges from the function above\n",
    "renamed_nodes, rename_map, renamed_hidden_nodes = rename_hidden_nodes(nodes)\n",
    "\n",
    "# Update edges with the renamed nodes\n",
    "renamed_edges = [(rename_map.get(edge[0], edge[0]), rename_map.get(edge[1], edge[1])) for edge in task4_skeleton_cleaned['edges']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.663187Z",
     "iopub.status.busy": "2024-03-02T21:32:17.663131Z",
     "iopub.status.idle": "2024-03-02T21:32:17.664667Z",
     "shell.execute_reply": "2024-03-02T21:32:17.664482Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the renamed map to update the column name of our expanded_matrix\n",
    "expanded_matrix = expanded_matrix.rename(columns=rename_map, index=rename_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- have renamed_nodes as V\n",
    "- have renamed_edges as E \n",
    "- use expanded_matrix to indicate independence relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.665770Z",
     "iopub.status.busy": "2024-03-02T21:32:17.665709Z",
     "iopub.status.idle": "2024-03-02T21:32:17.667201Z",
     "shell.execute_reply": "2024-03-02T21:32:17.666997Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_edges(edges):\n",
    "    cleaned_edges = set()\n",
    "    \n",
    "    for edge in edges:\n",
    "        # Sort the edge tuple to maintain consistency\n",
    "        cleaned_edge = tuple(sorted(edge))\n",
    "        cleaned_edges.add(cleaned_edge)\n",
    "    \n",
    "    return list(cleaned_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.668242Z",
     "iopub.status.busy": "2024-03-02T21:32:17.668182Z",
     "iopub.status.idle": "2024-03-02T21:32:17.669605Z",
     "shell.execute_reply": "2024-03-02T21:32:17.669407Z"
    }
   },
   "outputs": [],
   "source": [
    "renamed_edges_clean = clean_edges(renamed_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.670679Z",
     "iopub.status.busy": "2024-03-02T21:32:17.670611Z",
     "iopub.status.idle": "2024-03-02T21:32:17.673489Z",
     "shell.execute_reply": "2024-03-02T21:32:17.673309Z"
    }
   },
   "outputs": [],
   "source": [
    "def orient_edges_debug(nodes, edges, correlation_matrix):\n",
    "    is_additional_edges_are_oriented = True\n",
    "    E_hat = set()\n",
    "    \n",
    "    while is_additional_edges_are_oriented:\n",
    "        is_additional_edges_are_oriented = False\n",
    "        action_happened = False\n",
    "        \n",
    "        for edge1 in edges:\n",
    "            for edge2 in edges:\n",
    "                if edge1 != edge2:\n",
    "                    common_node = set(edge1).intersection(set(edge2))\n",
    "                    if len(common_node) == 1:\n",
    "                        common_node = common_node.pop()\n",
    "                        node1, node2 = set(edge1).difference({common_node}), set(edge2).difference({common_node})\n",
    "                        node1, node2 = node1.pop(), node2.pop()\n",
    "\n",
    "                        try:\n",
    "                            if correlation_matrix.loc[node1, node2] == 0:\n",
    "                                if (node1, common_node) not in E_hat and (common_node, node1) not in E_hat:\n",
    "                                    E_hat.add((node1, common_node))\n",
    "                                    action_happened = True\n",
    "                                if (node2, common_node) not in E_hat and (common_node, node2) not in E_hat:\n",
    "                                    E_hat.add((node2, common_node))\n",
    "                                    action_happened = True\n",
    "                            elif (node1, common_node) in E_hat or (common_node, node1) in E_hat:\n",
    "                                if (common_node, node2) not in E_hat and (node2, common_node) not in E_hat:\n",
    "                                    E_hat.add((common_node, node2))\n",
    "                                    action_happened = True\n",
    "                        except KeyError as e:\n",
    "                            print(f\"KeyError occurred: {e}\")\n",
    "                            # Handle the error or continue\n",
    "        \n",
    "        if action_happened:\n",
    "            is_additional_edges_are_oriented = True\n",
    "    \n",
    "    return E_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.674605Z",
     "iopub.status.busy": "2024-03-02T21:32:17.674527Z",
     "iopub.status.idle": "2024-03-02T21:32:17.675909Z",
     "shell.execute_reply": "2024-03-02T21:32:17.675704Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.676943Z",
     "iopub.status.busy": "2024-03-02T21:32:17.676880Z",
     "iopub.status.idle": "2024-03-02T21:32:17.679631Z",
     "shell.execute_reply": "2024-03-02T21:32:17.679443Z"
    }
   },
   "outputs": [],
   "source": [
    "def orient_edges(edges, correlation_matrix):\n",
    "    \n",
    "    is_additional_edges_are_oriented = True\n",
    "    E_hat = set()\n",
    "    \n",
    "    while is_additional_edges_are_oriented:\n",
    "        is_additional_edges_are_oriented = False\n",
    "        action_happened = False\n",
    "\n",
    "    for edge1, edge2 in combinations(edges, 2):\n",
    "        common_node = set(edge1).intersection(set(edge2))\n",
    "\n",
    "        if len(common_node) == 1:\n",
    "            common_node = common_node.pop()\n",
    "            node1, node2 = set(edge1).difference({common_node}), set(edge2).difference({common_node})\n",
    "            node1, node2 = node1.pop(), node2.pop()\n",
    "            \n",
    "            if correlation_matrix.loc[node1, node2] == 0 or correlation_matrix.loc[node2, node1] == 0:\n",
    "                if (node1, common_node) not in E_hat:\n",
    "                    E_hat.add((node1, common_node))\n",
    "                    action_happened = True\n",
    "                if (node2, common_node) not in E_hat:\n",
    "                    E_hat.add((node2, common_node))\n",
    "                    action_happened = True\n",
    "\n",
    "            elif (node1, common_node) in E_hat or (common_node, node2) in edges:\n",
    "                if (common_node, node2) not in E_hat:\n",
    "                    E_hat.add((common_node, node2))\n",
    "                    action_happened = True\n",
    "        \n",
    "        if action_happened:\n",
    "            is_additional_edges_are_oriented = True\n",
    "    \n",
    "    return E_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.680765Z",
     "iopub.status.busy": "2024-03-02T21:32:17.680706Z",
     "iopub.status.idle": "2024-03-02T21:32:17.682619Z",
     "shell.execute_reply": "2024-03-02T21:32:17.682431Z"
    }
   },
   "outputs": [],
   "source": [
    "E_hat = orient_edges(renamed_edges_clean, expanded_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.683716Z",
     "iopub.status.busy": "2024-03-02T21:32:17.683632Z",
     "iopub.status.idle": "2024-03-02T21:32:17.685414Z",
     "shell.execute_reply": "2024-03-02T21:32:17.685182Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_bidirectional_edges(E_hat):\n",
    "    bidirectional_edges = set()\n",
    "    for edge in E_hat:\n",
    "        if (edge[1], edge[0]) in E_hat:\n",
    "            bidirectional_edges.add(edge)\n",
    "            bidirectional_edges.add((edge[1], edge[0]))\n",
    "    if bidirectional_edges:\n",
    "        return bidirectional_edges\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.686413Z",
     "iopub.status.busy": "2024-03-02T21:32:17.686352Z",
     "iopub.status.idle": "2024-03-02T21:32:17.688247Z",
     "shell.execute_reply": "2024-03-02T21:32:17.688061Z"
    }
   },
   "outputs": [],
   "source": [
    "check_bidirectional_edges_result = check_bidirectional_edges(E_hat)\n",
    "check_bidirectional_edges_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.689339Z",
     "iopub.status.busy": "2024-03-02T21:32:17.689276Z",
     "iopub.status.idle": "2024-03-02T21:32:17.691762Z",
     "shell.execute_reply": "2024-03-02T21:32:17.691544Z"
    }
   },
   "outputs": [],
   "source": [
    "def type_2_detection(E_hat, nodes_list, check_bidirectional_edges_result):\n",
    "    \n",
    "    nodes = set(nodes_list)  # Convert the list to a set\n",
    "    max_node = max(nodes, key=lambda x: int(x[1:]))\n",
    "    max_index = int(max_node[1:])\n",
    "\n",
    "    if check_bidirectional_edges_result:\n",
    "        \n",
    "        for edge in check_bidirectional_edges_result:\n",
    "            E_hat.remove(edge)\n",
    "\n",
    "        unique_bidirectional_edges = set()\n",
    "\n",
    "        for edge in check_bidirectional_edges_result:\n",
    "            if (edge[1], edge[0]) not in unique_bidirectional_edges:\n",
    "                unique_bidirectional_edges.add(edge)\n",
    "\n",
    "        for edge in unique_bidirectional_edges:\n",
    "            max_index += 1\n",
    "            new_node = 'y' + str(max_index)\n",
    "            nodes.add(new_node)\n",
    "            E_hat.add((new_node, edge[0]))\n",
    "            E_hat.add((new_node, edge[1]))\n",
    "            renamed_hidden_nodes.add(new_node)\n",
    "    \n",
    "    else:\n",
    "        return list(nodes), E_hat\n",
    "\n",
    "    return list(nodes), E_hat  # Convert the set back to a list if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.692943Z",
     "iopub.status.busy": "2024-03-02T21:32:17.692874Z",
     "iopub.status.idle": "2024-03-02T21:32:17.694354Z",
     "shell.execute_reply": "2024-03-02T21:32:17.694162Z"
    }
   },
   "outputs": [],
   "source": [
    "nodes_final, E_hat_final = type_2_detection(E_hat, renamed_nodes, check_bidirectional_edges_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.695460Z",
     "iopub.status.busy": "2024-03-02T21:32:17.695387Z",
     "iopub.status.idle": "2024-03-02T21:32:17.697504Z",
     "shell.execute_reply": "2024-03-02T21:32:17.697311Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_directed_graph(nodes, directed_edges, renamed_hidden_nodes):\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for node in nodes:\n",
    "        G.add_node(node)\n",
    "    \n",
    "    # Add directed edges\n",
    "    for edge in directed_edges:\n",
    "        G.add_edge(*edge)\n",
    "    \n",
    "    # Create a list of node colors based on whether they are in renamed_hidden_nodes\n",
    "    node_colors = ['lightpink' if node in renamed_hidden_nodes else 'skyblue' for node in G.nodes()]\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n",
    "    nx.draw(G, pos, with_labels=True, arrows=True, node_size=700, node_color=node_colors, font_size=10, font_color='black', font_weight='bold', arrowsize=20, connectionstyle='arc3,rad=0.1')\n",
    "    plt.show()\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.698581Z",
     "iopub.status.busy": "2024-03-02T21:32:17.698503Z",
     "iopub.status.idle": "2024-03-02T21:32:17.928271Z",
     "shell.execute_reply": "2024-03-02T21:32:17.928035Z"
    }
   },
   "outputs": [],
   "source": [
    "G_1 = plot_directed_graph(nodes_final, E_hat_final, renamed_hidden_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.929597Z",
     "iopub.status.busy": "2024-03-02T21:32:17.929527Z",
     "iopub.status.idle": "2024-03-02T21:32:17.931770Z",
     "shell.execute_reply": "2024-03-02T21:32:17.931540Z"
    }
   },
   "outputs": [],
   "source": [
    "df_edge = pd.read_csv(original_edge_data_path)\n",
    "# df_edge = pandas2ri.rpy2py(original_edge_data)\n",
    "df_edge = pd.DataFrame(df_edge)\n",
    "# df_edge.columns = ['source', 'target']\n",
    "df_edge['source'] = df_edge['source'].astype(str)\n",
    "df_edge['target'] = df_edge['target'].astype(str)\n",
    "\n",
    "G_original = nx.from_pandas_edgelist(df_edge, 'source', 'target', create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.933009Z",
     "iopub.status.busy": "2024-03-02T21:32:17.932919Z",
     "iopub.status.idle": "2024-03-02T21:32:17.935740Z",
     "shell.execute_reply": "2024-03-02T21:32:17.935341Z"
    }
   },
   "outputs": [],
   "source": [
    "number_hidden_recovered = len(renamed_hidden_nodes)\n",
    "\n",
    "number_nodes_total = df_edge.shape[0] + 1\n",
    "number_visible_nodes = len(df_data.columns)\n",
    "\n",
    "true_number_hidden = number_nodes_total - number_visible_nodes\n",
    "recover_ratio_hidden = number_hidden_recovered/true_number_hidden\n",
    "\n",
    "true_list_hidde_nodes = [str(number_nodes_total - i) for i in range(true_number_hidden)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:32:17.939513Z",
     "iopub.status.busy": "2024-03-02T21:32:17.939452Z",
     "iopub.status.idle": "2024-03-02T21:32:17.941504Z",
     "shell.execute_reply": "2024-03-02T21:32:17.941280Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(G_estimated, G_true):\n",
    "    \"\"\"\n",
    "    Calculate Structural Hamming Distance (SHD), Precision, and Recall between two graphs.\n",
    "    SHD is the number of edge disagreements between the two graphs.\n",
    "    Precision is the proportion of correctly identified edges out of all predicted edges.\n",
    "    Recall is the proportion of actual edges that were correctly identified.\n",
    "    \"\"\"\n",
    "    # Convert estimated graph edges to a format compatible with the true graph\n",
    "    edges_estimated = {(u[1:], v[1:]) if u.startswith('y') and v.startswith('y') else (u, v)\n",
    "                       for u, v in G_estimated.edges()}\n",
    "    print(edges_estimated)\n",
    "    \n",
    "    # No conversion needed for true graph edges if they are already in the correct format\n",
    "    edges_true = set(G_true.edges())\n",
    "    print(edges_true)\n",
    "\n",
    "    # Calculate SHD\n",
    "    shd = len(edges_estimated.symmetric_difference(edges_true))\n",
    "    \n",
    "    # Calculate components for Precision and Recall\n",
    "    true_positives = len(edges_estimated.intersection(edges_true))\n",
    "    false_positives = len(edges_estimated) - true_positives\n",
    "    false_negatives = len(edges_true) - true_positives\n",
    "\n",
    "    # Precision calculation\n",
    "    if true_positives + false_positives > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    else:\n",
    "        precision = 0  # To avoid division by zero when there are no positive predictions\n",
    "\n",
    "    # Recall calculation\n",
    "    if true_positives + false_negatives > 0:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    else:\n",
    "        recall = 0  # To avoid division by zero when there are no true positives or false negatives\n",
    "    \n",
    "    return shd, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the graph object into true and estimate to be more clear\n",
    "g_estimated = G_1.copy()\n",
    "g_true = G_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graph_with_hidden_nodes(g_graph, hidden_nodes):\n",
    "    # Make a copy of the graph\n",
    "    g_copy = g_graph.copy()\n",
    "\n",
    "    # Helper function to add bidirectional edges between a list of nodes and remove the original node\n",
    "    def add_bidirectional_edges_and_remove_node(node, children):\n",
    "        for node1, node2 in combinations(children, 2):\n",
    "            g_copy.add_edge(node1, node2)\n",
    "            g_copy.add_edge(node2, node1)\n",
    "        # Remove the original node and its edges to/from children\n",
    "        for child in children:\n",
    "            if g_copy.has_edge(node, child):\n",
    "                g_copy.remove_edge(node, child)\n",
    "            if g_copy.has_edge(child, node):  # If the graph allows for backward edges\n",
    "                g_copy.remove_edge(child, node)\n",
    "        g_copy.remove_node(node)\n",
    "\n",
    "    def process_node(node):\n",
    "        parents = list(g_copy.predecessors(node))\n",
    "        children = list(g_copy.successors(node))\n",
    "        \n",
    "        # Remove the node and its edges if it has one or no children\n",
    "        if not parents and len(children) <= 1:\n",
    "            g_copy.remove_node(node)\n",
    "            for child in children:\n",
    "                if g_copy.has_edge(node, child):\n",
    "                    g_copy.remove_edge(node, child)\n",
    "            return\n",
    "\n",
    "        # For nodes with multiple children but no parents, add bidirectional edges between children, then remove the node and its edges\n",
    "        if not parents and len(children) > 1:\n",
    "            add_bidirectional_edges_and_remove_node(node, children)\n",
    "            return\n",
    "        \n",
    "        # For nodes with parents, add edges from each parent to each child, then remove the node and its related edges\n",
    "        if parents:\n",
    "            for parent in parents:\n",
    "                for child in children:\n",
    "                    g_copy.add_edge(parent, child)\n",
    "            # After adding edges from parents to children, remove the hidden node and its edges\n",
    "            g_copy.remove_node(node)\n",
    "            for parent in parents:\n",
    "                if g_copy.has_edge(parent, node):\n",
    "                    g_copy.remove_edge(parent, node)\n",
    "                if g_copy.has_edge(node, parent):  # If the graph allows for backward edges\n",
    "                    g_copy.remove_edge(node, parent)\n",
    "            for child in children:\n",
    "                if g_copy.has_edge(node, child):\n",
    "                    g_copy.remove_edge(node, child)\n",
    "                if g_copy.has_edge(child, node):  # If the graph allows for backward edges\n",
    "                    g_copy.remove_edge(child, node)\n",
    "    \n",
    "    for hidden_node in hidden_nodes:\n",
    "        if hidden_node in g_copy.nodes():\n",
    "            process_node(hidden_node)\n",
    "\n",
    "    g_copy = nx.DiGraph(g_copy)\n",
    "    \n",
    "    return g_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_graph_true = update_graph_with_hidden_nodes(g_true, true_list_hidde_nodes)\n",
    "updated_graph_estimated = update_graph_with_hidden_nodes(g_estimated, renamed_hidden_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges_esitimation = updated_graph_estimated.number_of_edges()\n",
    "num_edges_true = updated_graph_true.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shd_value, precision, recall  = calculate_metrics(updated_graph_estimated, updated_graph_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming shd_value and recover_ratio_hidden are defined earlier in your notebook\n",
    "algorithm_results = {\n",
    "    \"shd\": shd_value,  \n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"percentage_hidden_nodes_recovered\": recover_ratio_hidden,\n",
    "    \"number_of_edges_esitimation\":num_edges_esitimation,\n",
    "    \"number_of_edges_true\":num_edges_true\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly outputting JSON string\n",
    "print(\"algorithm_result\", json.dumps(algorithm_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
